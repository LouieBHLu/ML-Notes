## 监督学习

- 成对的输入和预期输出提供给算法
- 一般分为两类问题：**分类**与**回归**问题

## 无监督学习

- 只有输入数据是已知的，没有为算法提供输出数据
- Examples: 确定一系列博客文章的主题，将客户分成具有相似偏好的群组，检测网站的异常访问模式



## 监督学习

- 分类：预测类别标签
  - 二分类：正类和反类
- 回归：预测一个连续值或浮点数（根据教育水平、年龄预测一个人的年收入）
- 泛化：对没见过的数据做出精准预测
- 过拟合：构建一个对现有信息量来说过于复杂的模型 -> 预测训练集表现好，不能很好泛化到测试集中
- 欠拟合：选择过于简单的模型被称为欠拟合
- 收集更多数据，适当构建更复杂的模型，对监督学习任务往往更加有用



### 一般步骤

1. 根据数据建立样本数据集（mglearn）
2. 创建训练集与测试集（train_test_split）
3. 构建算法模型实例，并输入训练集数据（fit）
4. predict或score的方法测试测试集数据并验证模型可靠性



### 监督学习算法

#### 一些样本数据集

1. forge: 二分数据集，两个特征
2. wave: 回归数据集，两个特征
3. 威斯康星州乳腺癌数据集： 二分数据集，30个特征
4. 波士顿房价数据集：回归数据集， 13个特征
   1. 特征工程：将每两个特征的乘积作为新的特征（从已有特征导出新的特征）
5. 鸢尾花



#### K邻近

- K邻近分类
  - 邻居少 -> 模型过于复杂 -> 训练集精度高但是泛化程度不行
- K邻近回归
  - **$R^2$**: 决定系数，0到1之间，1 -> 完美预测，0 -> 常数模型
- 优点：模型容易理解
- 缺点：对于很多特征的数据集以及大多数特征取值都为0的数据集（**稀疏数据集**）效果不好



#### 线性模型

- 看做特征的加权求和



1. 线性回归（最小二乘法）

- 寻找w和b, 使得对训练集的预测值与真实的回归目标值y之间的均方误差最小 （https://zh.wikipedia.org/wiki/%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE）

- 无参数，无法控制模型的复杂度
- 在处理高维（多特征）数据集，存在**过拟合**问题 -> 引出**岭回归**



2. 岭回归

- 希望w尽量小，每一项接近0 -> 每个特征对输出的影响尽可能小

- **L2正则化**： 对模型做显式约束，以避免过拟合

- 调整alpha = 1.0以调整简单性和训练集性能二者对于模型的重要程度； 增大alpha会让W更加趋近0，正则化越强，降低训练集性能，可能会提高泛化性能

  

3. lasso

- **L1正则化**: 某些系数刚好为0 -> 有选择得忽略某些特征，或呈现某些重要的特征
- 训练集和测试集表现都很差 -> **欠拟合** -> 减小alpha，增加max_iter（运行迭代最大次数）的值
- **ElasticNet**: 结合lasso和Ridge，共两个参数



4. 用于分类的线性模型
- y > 0 作为分类标准
   1. Logistic回归（**可延伸到多类**）
   2. 线性支持向量机（**LinearSVC**）

- **C**：正则化强度的权衡参数； C越大，正则化越弱（和alpha正好相反）-> 模型更复杂 -> 避免欠拟合



5. 用于多分类的线性模型 

- **一对多余**：
  - 对每个类别都学习一个二分类模型，将此类别与所有其他类别分开
  - 由此生成了与类别个数一样多的二分类模型。在测试点上运行所有二分类模型进行预测，在对应类别上分数最高的分类器的类别标签返回作为结果



6. 优缺点与参数

- 参数

  - 正则化参数

    - 回归模型：alpha; 

    - LinearSVC，LogisticRegression：C

    - alpha越大或者C值越小：模型越简单，更泛型

  - 正则化

    - L1: 只有几个特征重要 -> 强可解释性， 但是容易**欠拟合**（忽略过多的特征）
    - L2: 默认

- 优点

  - 大型数据处理快，预测速度快，支持稀疏数据
  - - [ ] 研究LogisticRegression和Ridge模型的solver='sag'
  - 理解预测是如何进行的相对容易，但是难以解释**高度相关的特征及其系数**



#### 朴素贝叶斯分类器

- 和线性模型类似，训练速度快，但是泛化能力较弱
- 通过单独查看每个特征来学习参数，并从每个特征中收集简单的类别统计数据
- scikit-learn中的三种分类器
  - GaussionNB：任意连续数据
  - BernoulliNB：二分类数据，计算每个类别中每个特征不为0的元素个数
  - MultinomialNB：计数数据（每个特征代表某个对象的整数计数；**如一个单词在句子里出现的次数**）



#### 决策树

- 从一层层if/else中进行学习



1. 构造决策树

- 学习一系列if/else问题，以最快的速度得到正确答案：**测试**
- 对数据进行反复递归划分，直到每个区域，或者说每个**叶节点**都只包含单一目标值（单一类别或者单一回归值），或者说是**纯**叶节点。



2. 控制决策树的复杂度

- 构造到所有叶节点都是纯的 -> 过于复杂模型，**过拟合**
- 两种策略防止过拟合
  - **预剪枝**：提早停止树的生长；**限制树的最大深度、限制叶节点的最大数目、规定一个节点中数据点的最小数目**
  - **后剪枝**：先构造树，然后删除或折叠信息量少的node；
- scikit-learn中的实现（**只有预剪枝**）
  - DecisionTreeClassifier
  - DecisionTreeRegressor
  - 可以通过设置**max_depth = n**来减少树的最大深度



3. 分析决策树

- tree模块的export_graphviz函数来将树可视化
- 

## 无监督学习

### 无监督学习的类型

- **数据集无监督变换**：创建数据新的表示的算法，使数据更加容易被人或其他机器学习算法所理解。
  - 常见应用1：**降维**；将高维数据转为可视化的二位数据
  - 常见应用2：找到“构成”数据的各个组成部分；如对文本文档集合进行主题提取，可用于追踪社交媒体上的话题讨论
- **聚类**：将数据分成不同的组；与监督学习的分类不同的是，聚类算法并无标签，只是单纯讲类似或者距离接近的点分为一类，而不在乎具体的类别是什么



### 无监督学习的挑战

- 评估算法是否学到了有用的东西；由于算法一般用于不包含标签信息的数据 -> 不知道正确的输出 -> 难以评估表现
- 偏向应用于探索性的目的
- 数据**预处理**与**缩放**尤其重要



### 预处理与缩放

```python
mglearn.plots.plot_scaling()
```

#### 不同类型的预处理

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20201221185912137.png" alt="image-20201221185912137" style="zoom:50%;" />

- StandardScaler: 确保每个特征的平均值为0，方差为1
- RobustScaler: 使用中位数和四分位数位于同一范围，忽略很大不同的数据点**（异常值）**
- MinMaxScaler：移动数据使所有特征刚好位于0~1之间
- Normalizer：对每个数据点进行缩放，使得特征向量的欧式距离等于1



#### 聚类

- 将数据集划分成**簇**(clustering)的任务



1. k均值聚类

- 交替进行一下两个步骤
  - 将每个数据点分配给最近的**簇中心**
  - 讲每个**簇中心**设置为所分配的所有数据点的平均值
- 如果簇的分配不再发生变化，那么算法结束。
- **失败案例**
  - 每个簇都是凸型 -> 只能找到相对简单的形状
  - 假设所有簇在某种程度上具有相同的直径 -> 他总是将簇之间的边界刚好画在簇中心的中间文职
  - 假设所有方向对每个簇都同等重要 （分开的三部分被沿着对角线方向拉长）



2. 凝聚聚类

- 















  